{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bb467d-861d-4b07-a48d-8e5aa177c969",
   "metadata": {},
   "source": [
    "# Chat Extraction\n",
    "\n",
    "This benchmark combines classification, summarization, and extraction in one a combined task. The model is\n",
    "expected to output formatted json in the expected schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c401de19-814e-4bd7-bb9c-7ea6e217985c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "uid = uuid.uuid4().hex[:4]  # Avoid conflicts in project names\n",
    "\n",
    "# Get your API key from https://smith.langchain.com/settings\n",
    "api_keys = [\n",
    "    \"LANGCHAIN_API_KEY\"\n",
    "]\n",
    "for key in api_keys:\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f22779-a948-4833-8e8c-ace9ef17f56f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Chat Extraction already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/11aec5e3-a2f8-4942-b52a-2727dd9c10a0/datasets/1f457916-d48b-4989-af10-33ea06a3c8de.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name       </td><td>Chat Extraction                                                                                                                                            </td></tr>\n",
       "<tr><td>Type       </td><td>ExtractionTask                                                                                                                                             </td></tr>\n",
       "<tr><td>Dataset ID </td><td><a href=\"https://smith.langchain.com/public/00f4444c-9460-4a82-b87a-f50096f1cfef/d\" target=\"_blank\" rel=\"noopener\">00f4444c-9460-4a82-b87a-f50096f1cfef</a></td></tr>\n",
       "<tr><td>Description</td><td>A dataset meant to test the ability of an LLM to extract and infer\n",
       "structured information from a dialogue. The dialogue is between a user and a support\n",
       "engineer. Outputs should be structured as a JSON object and test both the ability\n",
       "of the LLM to correctly structure the information and its ability to perform simple \n",
       "classification tasks.                                                                                                                                                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "ExtractionTask(name='Chat Extraction', dataset_id='https://smith.langchain.com/public/00f4444c-9460-4a82-b87a-f50096f1cfef/d', description='A dataset meant to test the ability of an LLM to extract and infer\\nstructured information from a dialogue. The dialogue is between a user and a support\\nengineer. Outputs should be structured as a JSON object and test both the ability\\nof the LLM to correctly structure the information and its ability to perform simple \\nclassification tasks.', schema=<class 'langchain_benchmarks.extraction.tasks.chat_extraction.schema.GenerateTicket'>, instructions=ChatPromptTemplate(input_variables=['dialogue'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpdesk assistant responsible with extracting information and generating tickets. Dialogues are between a user and a support engineer.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['dialogue'], template='Generate a ticket for the following question-response pair:\\n<Dialogue>\\n{dialogue}\\n</Dialogue>'))]), dataset_url=None, dataset_name=None, eval_config=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "\n",
    "task = registry[\"Chat Extraction\"]\n",
    "\n",
    "# Clone the dataset to your tenant\n",
    "clone_public_dataset(task.dataset_id, dataset_name=task.name)\n",
    "\n",
    "\n",
    "task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1378a-9a62-477e-bdb8-a7fd10915b62",
   "metadata": {},
   "source": [
    "#### Schema\n",
    "\n",
    "Each extraction task has an expected output schema defined in a Pydantic BaseModel object, which we can use to\n",
    "get a JSON schema object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f30df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "# from typing import Optional, Dict, Any, List\n",
    "# import json\n",
    "# from langchain.llms.base import LLM\n",
    "# from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "# from pydantic import Field, BaseModel\n",
    "# from nu_extract_run import NuExtract\n",
    "\n",
    "# class LocalChatModel(LLM):\n",
    "#     tokenizer: Any = Field(exclude=True)\n",
    "#     model: Any = Field(exclude=True)\n",
    "#     model_name: str = Field(default=\"numind/NuExtract-1.5\")\n",
    "#     temperature: float = Field(default=0.0)\n",
    "#     device: str = Field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     bound_functions: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "#     function_call: Optional[str] = Field(default=None)\n",
    "#     nuextract = NuExtract()\n",
    "    \n",
    "#     def __init__(self, model_name: str = \"numind/NuExtract-1.5\", temperature: float = 0.0, device: str = None, pydantic_example = None, **kwargs):\n",
    "#         device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         model: Any = Field(exclude=True)\n",
    "#         # self.nuextract = NuExtract()\n",
    "#         self.pydantic_example = pydantic_example\n",
    "#         super().__init__(model_name=model_name, temperature=temperature, device=device, tokenizer=tokenizer, model=model, **kwargs)\n",
    "\n",
    "#     def bind_functions(self, functions: List[Dict[str, Any]], function_call: Optional[str] = None) -> LLM:\n",
    "#         new_instance = self.copy()\n",
    "#         new_instance.bound_functions = functions\n",
    "#         new_instance.function_call = function_call\n",
    "#         return new_instance\n",
    "\n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> str:\n",
    "#         prompt = f\"{prompt}\\nOutput only valid JSON\"\n",
    "#         # inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             output = self.nuextract.extract_json(self.pydantic_example, prompt)\n",
    "        \n",
    "#         print(output)\n",
    "\n",
    "#         meow\n",
    "\n",
    "#         # return \"{}\"\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"nuextract\"\n",
    "\n",
    "#     @property\n",
    "#     def _identifying_params(self) -> Dict[str, Any]:\n",
    "#         return {\"model_name\": self.model_name, \"temperature\": self.temperature, \"device\": self.device}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74e095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CarModel(BaseModel):\n",
    "#     Name: str\n",
    "#     Manufacturer: str\n",
    "#     Designers: list\n",
    "#     Number_of_units_produced: int\n",
    "\n",
    "# class DataModel(BaseModel):\n",
    "#     Car: CarModel\n",
    "# car_obj = DataModel(Car=CarModel(Name=\"\", Manufacturer=\"\", Designers=[], Number_of_units_produced=0))\n",
    "\n",
    "# model = LocalChatModel()\n",
    "\n",
    "# model(\"this si prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462f7b8-fd42-4613-ab5f-5f3cbbc37d28",
   "metadata": {},
   "source": [
    "## Define an extraction chain\n",
    "\n",
    "Let's build the extraction chain that we can use to get structured information from the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade7077c-4602-4e5b-ad6d-3eb43cbd0247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d877c3fab2445649da88a0a4ad46d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from transformers import pipeline\n",
    "from nu_extract_run import NuExtract\n",
    "\n",
    "def format_run(dialogue_input: dict):\n",
    "    question = dialogue_input[\"question\"]\n",
    "    answer = dialogue_input[\"answer\"]\n",
    "    return {\n",
    "        \"dialogue\": f\"<question>\\n{question}\\n</question>\\n\"\n",
    "        f\"<assistant-response>\\n{answer}\\n</assistant-response>\"\n",
    "    }\n",
    "\n",
    "llm = NuExtract()\n",
    "\n",
    "output_parser = JsonOutputFunctionsParser()\n",
    "extraction_chain = (\n",
    "    format_run\n",
    "    | task.instructions\n",
    "    | llm\n",
    "    | output_parser\n",
    "    | (lambda x: {\"output\": x})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a64f76-65ae-4367-b43f-f2be3431e7af",
   "metadata": {},
   "source": [
    "Now it's time to measure our chain's effectiveness!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821e4b0-8e67-418a-840c-470fcde42df0",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Let's evaluate the chain now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20857d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab7514e-a6ef-4c21-b90f-d9cbefcf5af1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'numind-NuExtract-1.5-bc96' at:\n",
      "https://smith.langchain.com/o/11aec5e3-a2f8-4942-b52a-2727dd9c10a0/datasets/1f457916-d48b-4989-af10-33ea06a3c8de/compare?selectedSessions=b891375a-1546-4710-b72f-42b71046487b\n",
      "\n",
      "View all tests for Dataset Chat Extraction at:\n",
      "https://smith.langchain.com/o/11aec5e3-a2f8-4942-b52a-2727dd9c10a0/datasets/1f457916-d48b-4989-af10-33ea06a3c8de\n",
      "[>                                                 ] 0/27"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 82a97a89-3480-4143-aaea-a158e2c1a642 with inputs {'answer': 'To use cache in ChatOpenAI in TypeScript, you can follow these steps:\\n\\n1. Import the necessary dependencies:\\n```typescript\\nimport { ChatOpenAI, RedisCache } from \\'langchain\\';\\n```\\n\\n2. Create an instance of the RedisCache class and pass it as the value for the `cache` parameter in the ChatOpenAI constructor:\\n```typescript\\nconst cache = new RedisCache(upstashRedisCache); // Replace `upstashRedisCache` with your Redis cache configuration\\nconst model = new ChatOpenAI({\\n  modelName: \"gpt-4-1106-preview\",\\n  streaming: true,\\n  cache: cache,\\n  callbacks: [\\n    {\\n      handleLLMNewToken(token) {\\n        doesToken = true;\\n        res.write(token);\\n      },\\n    },\\n  ],\\n});\\n```\\n\\nMake sure to replace `upstashRedisCache` with your Redis cache configuration.\\n\\nBy using the RedisCache class from the `langchain` library, you can enable caching in ChatOpenAI and improve the performance of your application.', 'question': 'const model = new ChatOpenAI({\\n      modelName: \"gpt-4-1106-preview\",\\n      streaming: true,\\n      cache: upstashRedisCache,\\n      callbacks: [\\n        {\\n          handleLLMNewToken(token) {\\n            doesToken = true\\n            res.write(token)\\n          },\\n        },\\n      ],\\n    })\\nhow to use cache in chatopenai in typescript?'}\n",
      "Error Type: ValidationError, Message: 1 validation error for Generation\n",
      "text\n",
      "  str type expected (type=type_error.str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meow\n",
      "System: You are a helpdesk assistant responsible with extracting information and generating tickets. Dialogues are between a user and a support engineer.\n",
      "Human: Generate a ticket for the following question-response pair:\n",
      "<Dialogue>\n",
      "<question>\n",
      "const model = new ChatOpenAI({\n",
      "      modelName: \"gpt-4-1106-preview\",\n",
      "      streaming: true,\n",
      "      cache: upstashRedisCache,\n",
      "      callbacks: [\n",
      "        {\n",
      "          handleLLMNewToken(token) {\n",
      "            doesToken = true\n",
      "            res.write(token)\n",
      "          },\n",
      "        },\n",
      "      ],\n",
      "    })\n",
      "how to use cache in chatopenai in typescript?\n",
      "</question>\n",
      "<assistant-response>\n",
      "To use cache in ChatOpenAI in TypeScript, you can follow these steps:\n",
      "\n",
      "1. Import the necessary dependencies:\n",
      "```typescript\n",
      "import { ChatOpenAI, RedisCache } from 'langchain';\n",
      "```\n",
      "\n",
      "2. Create an instance of the RedisCache class and pass it as the value for the `cache` parameter in the ChatOpenAI constructor:\n",
      "```typescript\n",
      "const cache = new RedisCache(upstashRedisCache); // Replace `upstashRedisCache` with your Redis cache configuration\n",
      "const model = new ChatOpenAI({\n",
      "  modelName: \"gpt-4-1106-preview\",\n",
      "  streaming: true,\n",
      "  cache: cache,\n",
      "  callbacks: [\n",
      "    {\n",
      "      handleLLMNewToken(token) {\n",
      "        doesToken = true;\n",
      "        res.write(token);\n",
      "      },\n",
      "    },\n",
      "  ],\n",
      "});\n",
      "```\n",
      "\n",
      "Make sure to replace `upstashRedisCache` with your Redis cache configuration.\n",
      "\n",
      "By using the RedisCache class from the `langchain` library, you can enable caching in ChatOpenAI and improve the performance of your application.\n",
      "</assistant-response>\n",
      "</Dialogue>\n",
      "meow\n",
      "[->                                                ] 1/27"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 4e631a5f-d21f-4508-bacf-24ca555f11fa with inputs {'answer': 'Yes, it is possible to use function calls with Llama models. Llama provides different wrappers and APIs that allow you to interact with the models using function calls. Here are a few examples:\\n\\n1. LlamaCpp: You can use the LlamaCpp wrapper to make function calls to Llama models. This wrapper provides a simple way to use Llama models by passing in a string and getting a string completion as the output [2].\\n\\n2. LlamaAPI: Llama also provides an API called LlamaAPI, which is a hosted version of Llama2 that supports function calling. You can use the LlamaAPI Python package to interact with Llama models by creating an instance of the LlamaAPI class and making function calls using the `invoke` method [1].\\n\\nThese wrappers and APIs allow you to easily integrate Llama models into your code and make function calls to generate responses or perform other tasks.', 'question': 'Is it possible to use function call with llama models'}\n",
      "Error Type: ValidationError, Message: 1 validation error for Generation\n",
      "text\n",
      "  str type expected (type=type_error.str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meow\n",
      "System: You are a helpdesk assistant responsible with extracting information and generating tickets. Dialogues are between a user and a support engineer.\n",
      "Human: Generate a ticket for the following question-response pair:\n",
      "<Dialogue>\n",
      "<question>\n",
      "Is it possible to use function call with llama models\n",
      "</question>\n",
      "<assistant-response>\n",
      "Yes, it is possible to use function calls with Llama models. Llama provides different wrappers and APIs that allow you to interact with the models using function calls. Here are a few examples:\n",
      "\n",
      "1. LlamaCpp: You can use the LlamaCpp wrapper to make function calls to Llama models. This wrapper provides a simple way to use Llama models by passing in a string and getting a string completion as the output [2].\n",
      "\n",
      "2. LlamaAPI: Llama also provides an API called LlamaAPI, which is a hosted version of Llama2 that supports function calling. You can use the LlamaAPI Python package to interact with Llama models by creating an instance of the LlamaAPI class and making function calls using the `invoke` method [1].\n",
      "\n",
      "These wrappers and APIs allow you to easily integrate Llama models into your code and make function calls to generate responses or perform other tasks.\n",
      "</assistant-response>\n",
      "</Dialogue>\n",
      "meow\n",
      "[--->                                              ] 2/27"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example e276069a-8ce7-43c5-93ba-e2c4801f66ae with inputs {'answer': 'To import the `langchain.dataframes` module, you can use the following code:\\n\\n```python\\nfrom langchain.dataframes import *\\n```\\n\\nThis will import all the classes and functions from the `langchain.dataframes` module, allowing you to use them in your code.', 'question': 'How do I import the module langchain.dataframes?'}\n",
      "Error Type: ValidationError, Message: 1 validation error for Generation\n",
      "text\n",
      "  str type expected (type=type_error.str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meow\n",
      "System: You are a helpdesk assistant responsible with extracting information and generating tickets. Dialogues are between a user and a support engineer.\n",
      "Human: Generate a ticket for the following question-response pair:\n",
      "<Dialogue>\n",
      "<question>\n",
      "How do I import the module langchain.dataframes?\n",
      "</question>\n",
      "<assistant-response>\n",
      "To import the `langchain.dataframes` module, you can use the following code:\n",
      "\n",
      "```python\n",
      "from langchain.dataframes import *\n",
      "```\n",
      "\n",
      "This will import all the classes and functions from the `langchain.dataframes` module, allowing you to use them in your code.\n",
      "</assistant-response>\n",
      "</Dialogue>\n",
      "meow\n",
      "[----->                                            ] 3/27"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 2ffc455c-fdfa-4f13-9438-b1cb0849f548 with inputs {'answer': 'Para crear un agente que pueda utilizar una base de datos vectorial existente y un documento en un directorio como fuente de información, puedes seguir estos pasos:\\n\\n1. Crear la base de datos vectorial:\\n   - Utiliza la clase `Chroma` del módulo `langchain.vectorstores` para crear la base de datos vectorial.\\n   - Carga los documentos en la base de datos utilizando un cargador de documentos, como `TextLoader` o `WebBaseLoader`.\\n   - Divide los documentos en fragmentos más pequeños utilizando `CharacterTextSplitter`.\\n   - Crea los vectores de los fragmentos utilizando un modelo de embeddings, como `OpenAIEmbeddings`.\\n   - Crea la base de datos vectorial utilizando el método `from_documents` de la clase `Chroma`.\\n\\n2. Crear el agente:\\n   - Importa las clases y funciones necesarias, como `initialize_agent`, `Tool`, `AgentType`, `LLMMathChain`, etc.\\n   - Define las herramientas del agente utilizando la clase `Tool`, especificando el nombre, la función y la descripción de cada herramienta.\\n   - Utiliza la función `initialize_agent` para construir el agente, pasando las herramientas, el modelo de lenguaje subyacente (por ejemplo, `OpenAI`), y el tipo de agente deseado (por ejemplo, `AgentType.ZERO_SHOT_REACT_DESCRIPTION`).\\n\\n3. Ejecutar el agente:\\n   - Utiliza el método `run` del agente para hacer una consulta al agente.\\n   - Proporciona la pregunta o consulta al agente como entrada.\\n   - El agente utilizará las herramientas y la base de datos vectorial para buscar la información relevante y proporcionar una respuesta.\\n\\nAquí tienes un ejemplo de código que muestra cómo crear un agente que utiliza una base de datos vectorial existente y un documento en un directorio como fuente de información:\\n\\n```python\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.document_loaders import TextLoader, WebBaseLoader\\nfrom langchain.agents import initialize_agent, Tool, AgentType\\n\\n# Crear la base de datos vectorial\\nllm = OpenAI(temperature=0)\\nloader = TextLoader(\"ruta_al_documento.txt\")\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nembeddings = OpenAIEmbeddings()\\nvector_db = Chroma.from_documents(texts, embeddings, collection_name=\"nombre_de_la_base_de_datos\")\\n\\n# Crear el agente\\ntools = [\\n    Tool(\\n        name=\"Herramienta de búsqueda\",\\n        func=vector_db.run,\\n        description=\"Utiliza la base de datos vectorial para buscar información relevante.\",\\n    )\\n]\\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\n\\n# Ejecutar el agente\\nconsulta = \"¿Cuál es la información relevante sobre un tema específico?\"\\nrespuesta = agent.run(consulta)\\nprint(respuesta)\\n```\\n\\nRecuerda reemplazar \"ruta_al_documento.txt\" con la ruta real al documento que deseas utilizar como fuente de información.', 'question': 'quiero crear un agente que sea capaz de usar una base de datos vectorial ya creada para buscar informacion, ademas que pueda usar un documento en un directorio como fuente de informacion para solucionar su proposito'}\n",
      "Error Type: ValidationError, Message: 1 validation error for Generation\n",
      "text\n",
      "  str type expected (type=type_error.str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meow\n",
      "System: You are a helpdesk assistant responsible with extracting information and generating tickets. Dialogues are between a user and a support engineer.\n",
      "Human: Generate a ticket for the following question-response pair:\n",
      "<Dialogue>\n",
      "<question>\n",
      "quiero crear un agente que sea capaz de usar una base de datos vectorial ya creada para buscar informacion, ademas que pueda usar un documento en un directorio como fuente de informacion para solucionar su proposito\n",
      "</question>\n",
      "<assistant-response>\n",
      "Para crear un agente que pueda utilizar una base de datos vectorial existente y un documento en un directorio como fuente de información, puedes seguir estos pasos:\n",
      "\n",
      "1. Crear la base de datos vectorial:\n",
      "   - Utiliza la clase `Chroma` del módulo `langchain.vectorstores` para crear la base de datos vectorial.\n",
      "   - Carga los documentos en la base de datos utilizando un cargador de documentos, como `TextLoader` o `WebBaseLoader`.\n",
      "   - Divide los documentos en fragmentos más pequeños utilizando `CharacterTextSplitter`.\n",
      "   - Crea los vectores de los fragmentos utilizando un modelo de embeddings, como `OpenAIEmbeddings`.\n",
      "   - Crea la base de datos vectorial utilizando el método `from_documents` de la clase `Chroma`.\n",
      "\n",
      "2. Crear el agente:\n",
      "   - Importa las clases y funciones necesarias, como `initialize_agent`, `Tool`, `AgentType`, `LLMMathChain`, etc.\n",
      "   - Define las herramientas del agente utilizando la clase `Tool`, especificando el nombre, la función y la descripción de cada herramienta.\n",
      "   - Utiliza la función `initialize_agent` para construir el agente, pasando las herramientas, el modelo de lenguaje subyacente (por ejemplo, `OpenAI`), y el tipo de agente deseado (por ejemplo, `AgentType.ZERO_SHOT_REACT_DESCRIPTION`).\n",
      "\n",
      "3. Ejecutar el agente:\n",
      "   - Utiliza el método `run` del agente para hacer una consulta al agente.\n",
      "   - Proporciona la pregunta o consulta al agente como entrada.\n",
      "   - El agente utilizará las herramientas y la base de datos vectorial para buscar la información relevante y proporcionar una respuesta.\n",
      "\n",
      "Aquí tienes un ejemplo de código que muestra cómo crear un agente que utiliza una base de datos vectorial existente y un documento en un directorio como fuente de información:\n",
      "\n",
      "```python\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.text_splitter import CharacterTextSplitter\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.document_loaders import TextLoader, WebBaseLoader\n",
      "from langchain.agents import initialize_agent, Tool, AgentType\n",
      "\n",
      "# Crear la base de datos vectorial\n",
      "llm = OpenAI(temperature=0)\n",
      "loader = TextLoader(\"ruta_al_documento.txt\")\n",
      "documents = loader.load()\n",
      "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
      "texts = text_splitter.split_documents(documents)\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vector_db = Chroma.from_documents(texts, embeddings, collection_name=\"nombre_de_la_base_de_datos\")\n",
      "\n",
      "# Crear el agente\n",
      "tools = [\n",
      "    Tool(\n",
      "        name=\"Herramienta de búsqueda\",\n",
      "        func=vector_db.run,\n",
      "        description=\"Utiliza la base de datos vectorial para buscar información relevante.\",\n",
      "    )\n",
      "]\n",
      "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
      "\n",
      "# Ejecutar el agente\n",
      "consulta = \"¿Cuál es la información relevante sobre un tema específico?\"\n",
      "respuesta = agent.run(consulta)\n",
      "print(respuesta)\n",
      "```\n",
      "\n",
      "Recuerda reemplazar \"ruta_al_documento.txt\" con la ruta real al documento que deseas utilizar como fuente de información.\n",
      "</assistant-response>\n",
      "</Dialogue>\n",
      "meow\n",
      "[------>                                           ] 4/27"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 21975f6c-c828-4c5c-83b1-af1e2b52d52e with inputs {'answer': 'To extract the logical argument from a text using OpenAI LLM (Large Language Model), there are several ways you can approach it. Here are some techniques you can use:\\n\\n1. **Causal Modeling**: Causal modeling involves analyzing the cause-and-effect relationships within the text to identify the logical argument. By understanding the logical flow of the text and identifying the premises and conclusions, you can extract the logical argument.\\n\\n2. **Robustness Testing**: Robustness testing involves subjecting the text to various scenarios and evaluating the consistency and validity of the logical argument. This can help identify any weaknesses or fallacies in the argument.\\n\\n3. **Bias Mitigation**: Bias mitigation techniques can be used to ensure that the logical argument is not influenced by any biased or unfair reasoning. By addressing any biases in the text, you can extract a more objective and logical argument.\\n\\nThe best way to extract the logical argument from a text using OpenAI LLM depends on the specific requirements and context of your task. It is recommended to combine multiple techniques and experiment with different approaches to find the most effective method for your use case.', 'question': 'I want to use openai LLM to extract the logical argument from a text. What are the ways to do it and what is the best way?'}\n",
      "Error Type: ValidationError, Message: 1 validation error for Generation\n",
      "text\n",
      "  str type expected (type=type_error.str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meow\n",
      "System: You are a helpdesk assistant responsible with extracting information and generating tickets. Dialogues are between a user and a support engineer.\n",
      "Human: Generate a ticket for the following question-response pair:\n",
      "<Dialogue>\n",
      "<question>\n",
      "I want to use openai LLM to extract the logical argument from a text. What are the ways to do it and what is the best way?\n",
      "</question>\n",
      "<assistant-response>\n",
      "To extract the logical argument from a text using OpenAI LLM (Large Language Model), there are several ways you can approach it. Here are some techniques you can use:\n",
      "\n",
      "1. **Causal Modeling**: Causal modeling involves analyzing the cause-and-effect relationships within the text to identify the logical argument. By understanding the logical flow of the text and identifying the premises and conclusions, you can extract the logical argument.\n",
      "\n",
      "2. **Robustness Testing**: Robustness testing involves subjecting the text to various scenarios and evaluating the consistency and validity of the logical argument. This can help identify any weaknesses or fallacies in the argument.\n",
      "\n",
      "3. **Bias Mitigation**: Bias mitigation techniques can be used to ensure that the logical argument is not influenced by any biased or unfair reasoning. By addressing any biases in the text, you can extract a more objective and logical argument.\n",
      "\n",
      "The best way to extract the logical argument from a text using OpenAI LLM depends on the specific requirements and context of your task. It is recommended to combine multiple techniques and experiment with different approaches to find the most effective method for your use case.\n",
      "</assistant-response>\n",
      "</Dialogue>\n",
      "meow\n",
      "[-------->                                         ] 5/27"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/runnables/config.py:555\u001b[0m, in \u001b[0;36mget_executor_for_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextThreadPoolExecutor(\n\u001b[1;32m    553\u001b[0m     max_workers\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_concurrency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    554\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m executor\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:1399\u001b[0m, in \u001b[0;36mrun_on_dataset\u001b[0;34m(client, dataset_name, llm_or_chain_factory, evaluation, dataset_version, concurrency_level, project_name, project_metadata, verbose, revision_id, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m runnable_config\u001b[38;5;241m.\u001b[39mget_executor_for_config(container\u001b[38;5;241m.\u001b[39mconfigs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m-> 1399\u001b[0m         batch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m_run_llm_or_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container\u001b[38;5;241m.\u001b[39mfinish(batch_results, verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[1;32m      8\u001b[0m eval_config \u001b[38;5;241m=\u001b[39m get_eval_config()\n\u001b[0;32m---> 10\u001b[0m test_run \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextraction_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# llm_or_chain_factory=llm,\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumind-NuExtract-1.5-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muid\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43march\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai-functions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal-NuExtract-1.5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langsmith/client.py:5137\u001b[0m, in \u001b[0;36mClient.run_on_dataset\u001b[0;34m(self, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, dataset_version, verbose, input_mapper, revision_id, **kwargs)\u001b[0m\n\u001b[1;32m   5132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   5133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   5134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe client.run_on_dataset function requires the langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage to run.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstall with pip install langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5136\u001b[0m     )\n\u001b[0;32m-> 5137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5146\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5149\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:1398\u001b[0m, in \u001b[0;36mrun_on_dataset\u001b[0;34m(client, dataset_name, llm_or_chain_factory, evaluation, dataset_version, concurrency_level, project_name, project_metadata, verbose, revision_id, **kwargs)\u001b[0m\n\u001b[1;32m   1388\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1389\u001b[0m         _run_llm_or_chain(\n\u001b[1;32m   1390\u001b[0m             example,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(container\u001b[38;5;241m.\u001b[39mexamples, container\u001b[38;5;241m.\u001b[39mconfigs)\n\u001b[1;32m   1396\u001b[0m     ]\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m runnable_config\u001b[38;5;241m.\u001b[39mget_executor_for_config(container\u001b[38;5;241m.\u001b[39mconfigs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   1399\u001b[0m         batch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   1400\u001b[0m             executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m   1401\u001b[0m                 functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1408\u001b[0m             )\n\u001b[1;32m   1409\u001b[0m         )\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container\u001b[38;5;241m.\u001b[39mfinish(batch_results, verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/runnables/config.py:552\u001b[0m, in \u001b[0;36mget_executor_for_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get an executor for a config.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m    Generator[Executor, None, None]: The executor.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    551\u001b[0m config \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextThreadPoolExecutor(\n\u001b[1;32m    553\u001b[0m     max_workers\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_concurrency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    554\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m executor\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 320f621f-fab5-412e-9154-e5ad827aba3b with inputs {'answer': 'To run Llama2 using pandas, you can follow these steps:\\n\\n1. Install the necessary packages:\\n   - Install the `langchain` library by running `pip install langchain`.\\n   - Install the `pandas` library by running `pip install pandas`.\\n\\n2. Import the required modules:\\n   - Import the `Ollama` class from the `langchain.llms` module.\\n   - Import the `pandas` module.\\n\\n3. Load your data into a pandas DataFrame:\\n   - Use the pandas library to load your data into a DataFrame. You can use functions like `read_csv()` or `read_excel()` depending on the format of your data.\\n\\n4. Create an instance of the `Ollama` class:\\n   - Initialize an instance of the `Ollama` class, specifying the desired model. For example, you can use `llm = Ollama(model=\"llama2\")`.\\n\\n5. Generate predictions:\\n   - Use the `llm` instance to generate predictions on your data. You can pass your data as input to the `llm()` method. For example, you can use `predictions = llm(data)`.\\n\\n6. Process the predictions:\\n   - Process the predictions returned by Llama2 according to your specific needs. You can use pandas DataFrame methods to manipulate and analyze the results.\\n\\nHere\\'s an example code snippet that demonstrates running Llama2 using pandas:\\n\\n```python\\nfrom langchain.llms import Ollama\\nimport pandas as pd\\n\\n# Load data into a pandas DataFrame\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Create an instance of the Ollama class\\nllm = Ollama(model=\"llama2\")\\n\\n# Generate predictions\\npredictions = llm(data)\\n\\n# Process the predictions\\n# ... (perform further analysis or manipulation using pandas DataFrame methods)\\n```\\n\\nPlease note that you may need to adapt the code to fit your specific use case and data format.', 'question': 'how do I run llama2 using pandas'}\n",
      "Error Type: IndentationError, Message: unexpected indent (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------->                                       ] 6/27"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example c1ab4fd9-a571-4bc1-bd4c-a3a45433f782 with inputs {'answer': \"To perform the task of creating an app that interacts with the user, converts user requests into structured descriptions (JSON), determines the action type, selects the appropriate agent or tool, and generates Python code based on API documentation, you can follow these steps using Langchain concepts:\\n\\n1. Set up the necessary components:\\n   - Define the tools: Identify the tools you need to interact with the user and perform the required actions. For example, you might need tools for user input, JSON manipulation, and API documentation retrieval.\\n   - Create the agent: Initialize an agent that can handle the user's requests, parse the inputs, and determine the appropriate actions to take based on the structured description.\\n\\n2. Implement the chat functionality:\\n   - Use a chat model to interact with the user: You can use a chat model from Langchain, such as the `ChatOpenAI` model, to handle the conversation with the user. This model can generate responses based on the user's inputs and the agent's instructions.\\n   - Define the prompts: Set up prompts that guide the conversation between the user and the chat model. These prompts should include placeholders for the user's inputs and the agent's scratchpad (intermediate steps).\\n\\n3. Convert user requests into structured descriptions:\\n   - Define the structured description format: Determine the format of the structured descriptions (JSON) that the agent will generate. This format should include the necessary fields to describe the action type, input data, and any other relevant information.\\n   - Use the agent to convert user requests: Configure the agent to parse the user's inputs and generate the structured descriptions based on the determined action type. The agent should use the available tools to manipulate and validate the JSON data.\\n\\n4. Determine the action type and select the appropriate agent or tool:\\n   - Implement logic to determine the action type: Based on the structured description generated by the agent, implement logic to determine the type of action required (e.g., create, search). This logic can be based on specific fields or patterns in the structured description.\\n   - Select the appropriate agent or tool: Depending on the determined action type, select the corresponding agent or tool to perform the action. For example, if the action is a search, you might use a search tool or agent to retrieve the desired information.\\n\\n5. Generate Python code based on API documentation:\\n   - Retrieve the API documentation: Access the specific API documentation required for performing the actions. This documentation should provide information about the available methods, parameters, and required inputs.\\n   - Use the agent to generate Python code: Utilize the agent's capabilities to generate Python code based on the structured description and the API documentation. The agent should be able to extract the relevant information from the documentation and construct the appropriate code snippets.\\n\\nBy following these steps and leveraging the Langchain concepts of tools, agents, chains, and actions, you can create an app that effectively interacts with the user, converts requests into structured descriptions, determines the action type, selects the appropriate agent or tool, and generates Python code based on API documentation.\", 'question': \"I want to create an app which:\\n- chats with user to retrieve all the data that user provides to perform a right type of action,\\n- when agent has all the needed data, it converts the user request into the structured description (JSON),\\n- in the structured description, agent determines which type of action is needed to satisfy user's request (types: create, search),\\n- depending on the action type determined by the agent, and based on the structurized action description, right agent or tool should be selected to proceed with action performance,\\n- for action performing, agent needs an access to the specific API documentation; using the API docs, agent will provide the python code (for invoking API proper methods) based on the action JSON description and API documentation.\\n\\nUsing langchain concepts, tools, chains, and actions - how should I perform this task?\"}\n",
      "Error Type: IndentationError, Message: unexpected indent (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------>                                     ] 7/27"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 28520bbe-6272-4c84-b79f-d31ac55c4574 with inputs {'answer': 'Pour joindre les deux outputs, vous pouvez utiliser la fonction `zip` pour combiner les éléments correspondants des deux listes, puis utiliser une boucle `for` pour les concaténer en une seule chaîne de caractères. Voici un exemple de code qui illustre cette approche :\\n\\n```python\\nheader_label = header.iloc[:, 1:].apply(lambda x: \\'/ \\'.join(x.dropna().astype(str).unique()), axis=1)\\nheader_list = \": \".join([header_filter, header_label])\\n\\noutput = \"\"\\nfor label, value in zip(header_label, header_list):\\n    output += f\"{label}: {value}\\\\n\"\\n\\nprint(output)\\n```\\n\\nCela devrait vous donner une sortie qui combine les éléments des deux listes dans un format spécifié.', 'question': 'je travail sur python. je souhaite joindre ces deux outputs : \\n\\n        header_label = header.iloc[:, 1:].apply(lambda x: \\'/ \\'.join(x.dropna().astype(str).unique()), axis=1)\\n        print(header_label)\\n        header_list = \": \".join([header_filter, header_label])\\n        print(header_list)\\n0                  FREQ_label\\n1          AME_REF_AREA_label\\n2    AME_TRANSFORMATION_label\\n3        AME_AGG_METHOD_label\\n4              AME_UNIT_label\\n5         AME_REFERENCE_label\\n6              AME_ITEM_label\\nName: Identifiant, dtype: object\\n0                                          Annual\\n1                                Austria/ Belgium\\n2        Original data and moving arithmetic mean\\n3                            Standard aggregation\\n4                               National currency\\n5                                    No reference\\n6    Gross domestic product at 2010 market prices\\ndtype: object\\n\\nObtenu à partir de ce code : '}\n",
      "Error Type: IndentationError, Message: unexpected indent (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------->                                   ] 8/27"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 7fba0db4-97b9-4bf3-b09f-84f41f0827e6 with inputs {'answer': 'To use a RecursiveUrlLoader to load content from a page, you need to follow these steps:\\n\\n1. Import the RecursiveUrlLoader class from the langchain.document_loaders.recursive_url_loader module.\\n2. Create an instance of the RecursiveUrlLoader, passing the target URL as the `url` parameter.\\n3. Optionally, you can customize the loader by specifying additional parameters such as `max_depth`, `exclude_dirs`, `use_async`, `extractor`, and `timeout`.\\n4. Call the `load()` method on the loader instance to load the web pages.\\n5. The `load()` method will return a list of Document objects, each representing a loaded web page.\\n6. You can access the content of each Document object using the `page_content` attribute.\\n\\nHere is an example of how to use the RecursiveUrlLoader:\\n\\n```python\\nfrom langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\\n\\nurl = \"https://docs.python.org/3.9/\"\\nloader = RecursiveUrlLoader(url=url, max_depth=2)\\ndocs = loader.load()\\n\\nfor doc in docs:\\n    print(doc.page_content)\\n```\\n\\nThis example loads web pages from the specified URL and prints the content of each page. You can customize the loader by modifying the parameters according to your needs.', 'question': 'How do I use a RecursiveUrlLoader to load content from a page?'}\n",
      "Error Type: IndentationError, Message: unexpected indent (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------->                                 ] 9/27"
     ]
    }
   ],
   "source": [
    "from langsmith.client import Client\n",
    "from nu_extract_run import NuExtract\n",
    "\n",
    "from langchain_benchmarks.extraction.tasks.chat_extraction import get_eval_config\n",
    "\n",
    "client = Client()\n",
    "\n",
    "eval_config = get_eval_config()\n",
    "\n",
    "test_run = client.run_on_dataset(\n",
    "    dataset_name=task.name,\n",
    "    llm_or_chain_factory=extraction_chain,\n",
    "    # llm_or_chain_factory=llm,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=f\"numind-NuExtract-1.5-{uid}\",\n",
    "    project_metadata={\n",
    "        \"arch\": \"openai-functions\",\n",
    "        \"model\": \"local-NuExtract-1.5\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9828990-f498-4d3f-9e51-76d72bf8f4e9",
   "metadata": {},
   "source": [
    "## Compare to Claude-2\n",
    "\n",
    "Let's compare our results to Anthropic's Claude-2. We will mimic the function calling interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9d1cb-b9b6-4d77-b0d5-63a6784626d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Type\n",
    "\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "\n",
    "claude_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a data extraction bot tasked with extracting and inferring information from dialogues and generating tickets. Always respond \"\n",
    "            \"only with XML based on the following JSON schema:\\n{schema}\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Generate a ticket from the following question-response pair:\\n\"\n",
    "            \"<Dialogue>\\n{dialogue}\\n</Dialogue>\\n\"\n",
    "            \"Remember, respond directly with this format:\\n\"\n",
    "            \"<{function_call}>\\n...\\n</{function_call}>\"\n",
    "            \"RESPOND ONLY IN XML THEN STOP.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "prompt = claude_prompt.partial(\n",
    "    schema=task.schema.schema_json(), function_call=task.schema.schema()[\"title\"]\n",
    ")\n",
    "\n",
    "claude = ChatAnthropic(model=\"claude-2\", temperature=0, max_tokens_to_sample=2048)\n",
    "\n",
    "\n",
    "class MergeSchema:\n",
    "    \"\"\"Merge the XML Output Parser schema into the output.\"\"\"\n",
    "\n",
    "    def __init__(self, schema: Type[BaseModel]):\n",
    "        self.schema = schema\n",
    "\n",
    "    @property\n",
    "    def _func_name(self) -> str:\n",
    "        return self.schema.__name__\n",
    "\n",
    "    def _merge_schema(self, parsed_output: Any, schema: Type[BaseModel]):\n",
    "        merged_output = {}\n",
    "        if isinstance(parsed_output, dict):\n",
    "            items = parsed_output.items()\n",
    "        elif isinstance(parsed_output, list):\n",
    "            items = [(k, v) for item in parsed_output for k, v in item.items()]\n",
    "        else:\n",
    "            return parsed_output\n",
    "\n",
    "        for key, value in items:\n",
    "            if key in schema.__fields__:\n",
    "                field_info = schema.__fields__[key]\n",
    "                if isinstance(value, list):\n",
    "                    if issubclass(field_info.type_, (BaseModel, dict)):\n",
    "                        result = self._merge_schema(value, field_info.type_)\n",
    "                    elif all(\n",
    "                        isinstance(item, dict) and item.keys() == {\"item\"}\n",
    "                        for item in value\n",
    "                    ):\n",
    "                        result = [next(iter(item.values())) for item in value]\n",
    "                    else:\n",
    "                        result = value\n",
    "                else:\n",
    "                    result = value\n",
    "            else:\n",
    "                result = value\n",
    "            if key in merged_output:\n",
    "                if isinstance(merged_output[key], list):\n",
    "                    merged_output[key].append(result)\n",
    "                else:\n",
    "                    merged_output[key] = [merged_output[key], result]\n",
    "            else:\n",
    "                merged_output[key] = result\n",
    "\n",
    "        return merged_output\n",
    "\n",
    "    def __call__(self, parsed_output: dict) -> Dict[str, Any]:\n",
    "        merged_output = {}\n",
    "        if self._func_name not in parsed_output:\n",
    "            return parsed_output\n",
    "        return {\n",
    "            self._func_name: self._merge_schema(\n",
    "                parsed_output[self._func_name], self.schema\n",
    "            )\n",
    "        }\n",
    "\n",
    "\n",
    "def try_parse(llm_output, config):\n",
    "    try:\n",
    "        output_chain = XMLOutputParser() | MergeSchema(task.schema)\n",
    "        parsed = output_chain.invoke(llm_output, config)\n",
    "        # Wrap as 'output' so to be unified for the evaluators\n",
    "        return {\"output\": parsed.get(\"GenerateTicket\")}\n",
    "    except Exception as e:\n",
    "        return {\"output\": llm_output, \"error\": str(e)}\n",
    "\n",
    "\n",
    "claude_extraction_chain = format_run | prompt | claude | try_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea759e7-a51a-4abd-9869-f928bea80da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = claude_extraction_chain.invoke(\n",
    "    {\"question\": \"how do i run llama 2 locally?\", \"answer\": \"Llama.cpp of course.\"}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723e6f4-b214-46a8-9286-93116fe893d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "claude_test_run = client.run_on_dataset(\n",
    "    dataset_name=task.name,\n",
    "    llm_or_chain_factory=claude_extraction_chain,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=f\"claude-2-json-schema-to-xml-{uid}\",\n",
    "    project_metadata={\n",
    "        \"arch\": \"claude-json-schema-xml-output\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34455c-e9d3-4fb0-b8d7-a3ee4a4b6ae0",
   "metadata": {},
   "source": [
    "So it looks like edit distance is pretty good, but the schema validation leaves something to be desired.\n",
    "\n",
    "We're defining the schema in JSON then requesting XML. Let's try keeping it unified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9612d56-08a1-4f24-a961-af7f7916997d",
   "metadata": {},
   "source": [
    "## Try with XSD Schema Definition\n",
    "\n",
    "In this variant, let's see if Claude performs better if we keep our structure consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9914571-d3f2-4f48-bdbb-2dfcfb03f26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Type\n",
    "\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "\n",
    "# This is the schema the model will populate\n",
    "xsd = \"\"\"<xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\">\n",
    "\n",
    "    <xs:simpleType name=\"QuestionCategory\">\n",
    "        <xs:restriction base=\"xs:string\">\n",
    "            <xs:enumeration value=\"Implementation Issues\"/>\n",
    "            <xs:enumeration value=\"Feature Requests\"/>\n",
    "            <xs:enumeration value=\"Concept Explanations\"/>\n",
    "            <xs:enumeration value=\"Code Optimization\"/>\n",
    "            <xs:enumeration value=\"Security and Privacy Concerns\"/>\n",
    "            <xs:enumeration value=\"Model Training and Fine-tuning\"/>\n",
    "            <xs:enumeration value=\"Data Handling and Manipulation\"/>\n",
    "            <xs:enumeration value=\"User Interaction Flow\"/>\n",
    "            <xs:enumeration value=\"Technical Integration\"/>\n",
    "            <xs:enumeration value=\"Error Handling and Logging\"/>\n",
    "            <xs:enumeration value=\"Customization and Configuration\"/>\n",
    "            <xs:enumeration value=\"External API and Data Source Integration\"/>\n",
    "            <xs:enumeration value=\"Language and Localization\"/>\n",
    "            <xs:enumeration value=\"Streaming and Real-time Processing\"/>\n",
    "            <xs:enumeration value=\"Tool Development\"/>\n",
    "            <xs:enumeration value=\"Function Calling\"/>\n",
    "            <xs:enumeration value=\"LLM Integrations\"/>\n",
    "            <xs:enumeration value=\"General Agent Questions\"/>\n",
    "            <xs:enumeration value=\"General Chit Chat\"/>\n",
    "            <xs:enumeration value=\"Memory\"/>\n",
    "            <xs:enumeration value=\"Debugging Help\"/>\n",
    "            <xs:enumeration value=\"Application Design\"/>\n",
    "            <xs:enumeration value=\"Prompt Templates\"/>\n",
    "            <xs:enumeration value=\"Cost Tracking\"/>\n",
    "            <xs:enumeration value=\"Other\"/>\n",
    "        </xs:restriction>\n",
    "    </xs:simpleType>\n",
    "\n",
    "    <xs:simpleType name=\"Sentiment\">\n",
    "        <xs:restriction base=\"xs:string\">\n",
    "            <xs:enumeration value=\"Negative\"/>\n",
    "            <xs:enumeration value=\"Neutral\"/>\n",
    "            <xs:enumeration value=\"Positive\"/>\n",
    "        </xs:restriction>\n",
    "    </xs:simpleType>\n",
    "\n",
    "    <xs:simpleType name=\"ProgrammingLanguage\">\n",
    "        <xs:restriction base=\"xs:string\">\n",
    "            <xs:enumeration value=\"python\"/>\n",
    "            <xs:enumeration value=\"javascript\"/>\n",
    "            <xs:enumeration value=\"typescript\"/>\n",
    "            <xs:enumeration value=\"unknown\"/>\n",
    "            <xs:enumeration value=\"other\"/>\n",
    "        </xs:restriction>\n",
    "    </xs:simpleType>\n",
    "\n",
    "    <xs:complexType name=\"QuestionCategorization\">\n",
    "        <xs:sequence>\n",
    "            <xs:element name=\"question_category\" type=\"QuestionCategory\"/>\n",
    "            <xs:element name=\"category_if_other\" type=\"xs:string\" minOccurs=\"0\"/>\n",
    "            <xs:element name=\"is_off_topic\" type=\"xs:boolean\"/>\n",
    "            <xs:element name=\"toxicity\" type=\"xs:int\">\n",
    "                <xs:minInclusive value=\"0\"/>\n",
    "                <xs:maxInclusive value=\"5\"/>\n",
    "            </xs:element>\n",
    "            <xs:element name=\"sentiment\" type=\"Sentiment\"/>\n",
    "            <xs:element name=\"programming_language\" type=\"ProgrammingLanguage\"/>\n",
    "        </xs:sequence>\n",
    "    </xs:complexType>\n",
    "\n",
    "    <xs:simpleType name=\"ResponseType\">\n",
    "        <xs:restriction base=\"xs:string\">\n",
    "            <xs:enumeration value=\"resolve issue\"/>\n",
    "            <xs:enumeration value=\"provide guidance\"/>\n",
    "            <xs:enumeration value=\"request information\"/>\n",
    "            <xs:enumeration value=\"give up\"/>\n",
    "            <xs:enumeration value=\"none\"/>\n",
    "            <xs:enumeration value=\"other\"/>\n",
    "        </xs:restriction>\n",
    "    </xs:simpleType>\n",
    "\n",
    "    <xs:complexType name=\"ResponseCategorization\">\n",
    "        <xs:sequence>\n",
    "            <xs:element name=\"response_type\" type=\"ResponseType\"/>\n",
    "            <xs:element name=\"response_type_if_other\" type=\"xs:string\" minOccurs=\"0\"/>\n",
    "            <xs:element name=\"confidence_level\" type=\"xs:int\">\n",
    "                <xs:minInclusive value=\"0\"/>\n",
    "                <xs:maxInclusive value=\"5\"/>\n",
    "            </xs:element>\n",
    "            <xs:element name=\"followup_actions\" type=\"xs:string\" minOccurs=\"0\" maxOccurs=\"unbounded\"/>\n",
    "        </xs:sequence>\n",
    "    </xs:complexType>\n",
    "\n",
    "    <xs:complexType name=\"GenerateTicket\">\n",
    "        <xs:sequence>\n",
    "            <xs:element name=\"issue_summary\" type=\"xs:string\"/>\n",
    "            <xs:element name=\"question\" type=\"QuestionCategorization\"/>\n",
    "            <xs:element name=\"response\" type=\"ResponseCategorization\"/>\n",
    "        </xs:sequence>\n",
    "    </xs:complexType>\n",
    "\n",
    "</xs:schema>\"\"\"\n",
    "\n",
    "prompt = claude_prompt.partial(schema=xsd, function_call=task.schema.schema()[\"title\"])\n",
    "\n",
    "claude_extraction_chain = format_run | prompt | claude | try_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc6d70-b745-4fd3-9592-1a13a3f2751f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = claude_extraction_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"how do i run llama 2 locally?\",\n",
    "        \"answer\": \"Llama.cpp of course. Afterwords remember to install it, then add it to your path!\",\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d58656-108d-48d2-ba16-815fc9bdebcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "claude_xsd_test_run = client.run_on_dataset(\n",
    "    dataset_name=task.name,\n",
    "    llm_or_chain_factory=claude_extraction_chain,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=f\"claude-2-xsd-to-xml-{uid}\",\n",
    "    project_metadata={\n",
    "        \"arch\": \"claude-xml\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7ce82-73a7-4913-9569-1066d982b528",
   "metadata": {},
   "source": [
    "The json schema metric went down, meaning that the output counter-intuitively is less friendly to our parser than before.\n",
    "\n",
    "\n",
    "Let's try with an open source model: `llama-v2-34b-code-instruct`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102df41d-2c93-4ffc-a09a-4198ea5b6acc",
   "metadata": {},
   "source": [
    "## Try with Llama 2\n",
    "\n",
    "`llama-v2-34b-code-instruct` is an open source model that is meant to be good at both code-gen and other tasks.\n",
    "Let's benchmark it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc37f1-2dc3-4d8e-a380-3c8296bf105a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain.chat_models import ChatFireworks\n",
    "from langchain.output_parsers.json import parse_json_markdown\n",
    "\n",
    "llama_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a data extraction bot tasked with extracting and inferring information from dialogues and generating tickets. Always respond only with json based on the following JSON schema:\\n{schema}\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Generate a ticket from the following question-response pair:\\n\"\n",
    "            \"<Dialogue>\\n{dialogue}\\n</Dialogue>\\n\"\n",
    "            \"Remember, respond directly with this format:\\n\"\n",
    "            '{{\"{function_call}\": ...}}\\n'\n",
    "            \"RESPOND ONLY IN JSON THEN STOP.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = llama_prompt.partial(\n",
    "    schema=task.schema.schema_json(), function_call=task.schema.schema()[\"title\"]\n",
    ")\n",
    "\n",
    "llm = ChatFireworks(\n",
    "    model=\"accounts/fireworks/models/llama-v2-34b-code-instruct\",\n",
    "    temperature=0,\n",
    "    model_kwargs={\"max_tokens\": 4000},\n",
    ")\n",
    "\n",
    "\n",
    "def parse_output(ai_message):\n",
    "    content = ai_message.content\n",
    "    parser = lambda x: json.loads(x, strict=False)\n",
    "    try:\n",
    "        parsed = parse_json_markdown(content, parser=parser)\n",
    "        if \"GenerateTicket\" in parsed:\n",
    "            return {\"output\": parsed[\"GenerateTicket\"]}\n",
    "        return {\"output\": parsed}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"output\": content}\n",
    "\n",
    "\n",
    "fireworks_extraction_chain = format_run | prompt | llm | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e2273-2fd7-42c2-986b-c08a07cbcc96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = fireworks_extraction_chain.invoke(\n",
    "    {\"question\": \"how do i run llama 2 locally?\", \"answer\": \"Llama.cpp of course.\"}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f4b39-d1b0-4f89-aa09-4fe261296dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_v2_test_run = client.run_on_dataset(\n",
    "    dataset_name=task.name,\n",
    "    llm_or_chain_factory=fireworks_extraction_chain,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=f\"llama-v2-34b-code-instruct-{uid}\",\n",
    "    project_metadata={\"arch\": \"claude-xml\", \"model\": \"llama-v2-34b-code-instruct\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b039225-01cf-481a-87a6-4e880e9b1dcd",
   "metadata": {},
   "source": [
    "## Compare Results\n",
    "\n",
    "Here, we'll take a look at the underlying results a little bit. You can review the results to see relative performance in aggregate and on a per-example basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb19db1-43b8-4866-a3d2-f211ba92ab8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    test_run.to_dataframe()\n",
    "    .join(claude_test_run.to_dataframe(), rsuffix=\"_claude\")\n",
    "    .join(claude_xsd_test_run.to_dataframe(), rsuffix=\"_claude_xsd\")\n",
    "    .join(llama_v2_test_run.to_dataframe(), rsuffix=\"_llama_v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292b4ed-8331-4068-82fa-7cea2725e24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da665f3c-4ef6-474d-8ab5-284434060bec",
   "metadata": {},
   "source": [
    "#### Here, we compare the aggregate metrics side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b936c2-d676-4931-bb13-ec06ab55d401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    test_run.get_aggregate_feedback()\n",
    "    .add_suffix(\".gpt-4\")\n",
    "    .join(claude_test_run.get_aggregate_feedback(), rsuffix=\".claude\")\n",
    "    .join(claude_xsd_test_run.get_aggregate_feedback(), rsuffix=\".claude_xsd\")\n",
    "    .join(llama_v2_test_run.get_aggregate_feedback(), rsuffix=\".llama_v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a151781-9c69-43c3-84d7-5617ee0e7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "feedback_columns = sorted(\n",
    "    {col.rsplit(\".\", 1)[0] for col in df.columns if col.startswith(\"feedback.\")}\n",
    ")\n",
    "\n",
    "\n",
    "def render_metric(df, metric):\n",
    "    sub_cols = [col for col in df.columns if col.startswith(metric)]\n",
    "    display(HTML(f\"<h3>{metric.split('.')[-1]}</h3>\"))\n",
    "    display(df[sub_cols][df.index.isin([\"mean\", \"std\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97892d06-ac72-43fa-8e1e-ff33b284940d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feedback_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090284d7-29b6-4ea7-b193-ebc159fae143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_metric(df, \"execution_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4cf5f5-dd75-4318-9bf4-25b63fa1b895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for metric in feedback_columns:\n",
    "    render_metric(df, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1641d5b-362d-4aae-9f42-ccb4726b8229",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Try it out yourself! You can see some additional experiments on Open Source models in [this repo](https://github.com/hinthornw/llama-extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"numind/NuExtract-1.5\"\n",
    "summarizer = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to run the evaluation\n",
    "def evaluate_model(data, summarizer):\n",
    "    results = []\n",
    "    for index, row in data.iterrows():\n",
    "        if index < 10:\n",
    "            dialogue = f\"<question>\\n{row['question']}\\n</question>\\n<assistant-response>\\n{row['answer']}\\n</assistant-response>\"\n",
    "            summary = summarizer(dialogue, max_length=150, min_length=30, do_sample=False)\n",
    "            results.append(summary[0]['summary_text'])\n",
    "    return results\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_results = evaluate_model(df, summarizer)\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5678ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f9b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maaya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
